# O-нотация

## Описание

Big O, Big Omega, Big Theta - это математические обозначения для асимптотического поведения функций. Используются в
математическом анализе и теории алгоритмов. Под асимптотикой понимается характер изменения функции при стремлении её
аргумента к бесконечно большому числу.

Формальное определение всех O нотаций:
![img_5.png](../assets/img_5.png)

## Big O и алгоритмы

При оценке алгоритмов нам понадобится Big-O, чтобы определять эффективность алгоритма в плане скорости работы или
требуемой дополнительной памяти. Важно понять, что эти обозначения не сообщают *точной* времени работы алгоритма - они
лишь показывают *порядок роста (order of growth)* времени или памяти в зависимости от входных данных.

Таким образом, Big-O разделяет алгоритмы на классы эффективности, например: `O(log n)`, `O(n)`, `O(n * log n)`, `O(n^2)`
и так далее. Здесь n - это размер входных данных. Например, мы говорим, что линейный поиск (linear search) относится к
классу `O(n)`, потому что его время работы возрастает линейно в зависимости от входных данных. Если мы увеличим размер
входных данных, например, с n до 2n, то и время работы вырастет в 2 раза. А вот если алгоритм работает за время `O(n^2)`
, то при увеличении n до 2n, время работы вырастет в 4 раза.

Как же нам оценить время работы алгоритма в терминах Big-O? Для этого нам нужно составить функцию, которая считает
точное количество операций алгоритма.

Например, в Википедии хорошо разобран алгоритм "Сортировка вставками" - приведу скриншот из статьи.

<details>
    <summary>Оценка количества операций для сортировки вставками</summary>

![img.png](../assets/insertion_sort_efficiency_function.png)
</details>

## Отбрасывание non-dominant слагаемых

Как я уже сказал, используя асимптотические обозначения, мы хотим показать *порядок роста* функции. Что это значит на
деле? Это значит, что мы отбрасываем:

1. Не доминирующие (non-dominant) слагаемые из функции, в том числе константы
3. Константные множители (constants)

Например, если мы оценили, что количество операций алгоритма можно описать такой функцией: `f(n)=5n^2 + n + 10`, то в
итоге мы равно говорим, что эта функциия принадлежит к классу `O(n^2)`. Почему же так?

Дело в том, что на достаточно больших входных данных все эти множители и не доминирующие слагаемые *перестают играть
значимую роль*. А асимптотика дает оценку роста функции именно при стремлении ее аргумента к бесконечному числу.

Например, давайте сравним рост функций `f(x)=x+log(x)+16` и `g(x)=x`. Это на маленьких X:

<img src="assets/desmos-graph-growth-low-n.png" alt="growth-big-n" width="400" height="400"/>

Да, первая функция пока что обходит вторую. А теперь давайте возьмем X побольше и посмотрим на их рост еще раз:

<img src="assets/desmos-graph-growth-big-n.png" alt="growth-big-n" width="400" height="400"/>

Мы видим, что их графики уже сравнялись и совершенно не отличимы. Да, конечно, первая функция будет все еще больше, но
вопрос в том, насколько значительно. Здесь мы понимаем, что на достаточно больших n основную роль играет только
доминирующее слагаемое из всей функции.

## Логарифм в асимптотике

Когда мы даем асимптотическую оценку, мы не указываем основание логарифма.

Это потому, что нам важно только поведение функции: константное, логарифмическое или линейное. Не важно, какое основание
логарифма, важно то, что логарифмическое время всегда лучше линейного.

---

## Omega-O

Мы используем Big-Omega (Big-Ω), когда хотим сказать асимптотически *нижнюю* границу роста функции, то есть алгоритм
займет *как минимум* столько-то времени.

### Формальное определение

`f(n)=Ω(g(n))` тогда и только тогда, когда существует некоторая положительная константа `c` и некоторое неотрицательное
число `n_0`, такие что `f(n) >= c*g(n)` для всех `n >= n_0`.

То есть, если сложность алгоритма Ω(g(n)), то для достаточно больших n время выполнения займет как минимум c * g(n).

![img_6.png](../assets/img_6.png)

## Theta-O. Asymptotically Tight Bound

Мы используем Big-Theta (Big-θ), когда хотим сказать *асимптотически точную* сложность алгоритма (asymptotically tight
bound).

Так, если сложность алгоритма Ω(g(n)), то для достаточно больших n время выполнения займет как минимум c1 * g(n) и как
максимум c2 * g(n) времени для некоторых констант c1 и c2.

Верно, что если `f(n) = θ(g(n))`, то `f(n) = Ω(g(n))` и `f(n) = O(g(n))`. И верно обратное: если `f(n) = Ω(g(n))`
и `f(n) = O(g(n))`, то `f(n) = θ(g(n))`.

![img_7.png](../assets/img_7.png)

## Big-O

Мы используем Big-O, когда хотим сказать асимптотически *верхнюю* границу роста функции, то есть алгоритм займет как
максимум столько-то времени.

Зачастую мы используем как раз Big-O, так как нам интересно только максимальное время работы алгоритма - не путать с
лучшим и худшим случаем входных данных.

Например, мы знаем что в худшем случае бинарный поиск работает за время Θ(log2n). Но нельзя сказать что бинарный поиск
работает за Θ(log2n) во всех случаях, включая лучшие случаи. Ведь мы можем найти искомое число и на первой итерации.
Тогда это было бы Θ(1). То есть время выполнения алгоритма никогда не хуже, чем Θ(log2n), но иногда лучше.

То есть, используя Big-O, мы хотим сказать: время выполнения растет как максимум таким образом, но может расти и
медленнее.

### Формальное определение

`f(n)=O(g(n))` тогда и только тогда, когда существует некоторая положительная константа `c` и некоторое неотрицательное
число `n_0`, такие что `f(n) <= c*g(n)` для всех `n >= n_0`.

Так, если время выполнения O(g(n)), то для достаточно больших n время выполнения займет как максимум c * g(n).

![img_8.png](../assets/img_8.png)

---

## Sharp bound

Верно, что время работы бинарного поиска можно описать как O(n^2) или как O(n). Но дело в том, что это будут не
максимально точные асимптотические оценки.

Когда мы используем Big-O нотации, мы стараемся давать максимально точные асимптотические оценки (sharp bound, tightest
bound). То есть такую оценку, которая точно ограничивает время работы функции f снизу или сверху в пределах некоторой
константы, но по росту максимально близка к рассматриваемой функции и эту оценку нельзя улучшить.

---

## Little-O, Little-Omega

Существуют Little-O ("o") и Little-Omega ("ω"). Они отличаются тем, что в их формальном определении знак сравнения
строгий и необходимо выполнение условия при любой константе C, а не некоторой. Little-Theta не существует.

Такие нотации мы используем, когда не можем сказать точную нижнюю или верхнюю границу (loose bound). То есть
сравниваемая функция f(n) всегда будет расти строго быстрее или медленнее, чем g(n). Например, справедливо что время
выполнения бинарного поиска - o(n) и ω(1).

---

## Пределы

Можно рассмотреть O-нотации с точки зрения пределов.

Если взять две функции f(n) и g(n) при n стремящемся к бесконечности, то предел отношения f(n)/g(n):

- равен 0, если f(n) имеет меньший порядок роста, чем g(n)
- равен c, если f(n) имеет тот же порядок роста, что и g(n)
- равен бесконечности, если f(n) имеет больший порядок роста, чем g(n)

Тогда для Big-O нотаций верно:

1. Для двух первых случаев `f(n) = O(g(n))`
2. Для двух последних случаев `f(n) = Ω(g(n))`
3. Для второго случая `f(n) = θ(g(n))`

А для Little-O нотаций верно:

1. Для первого случая `f(n) - o(g(n))`
2. для третьего случая `f(n) - ω(g(n))`

Это я взял из книги - вот выдержка из нее:
![](assets/Порядок_роста_функций_(пределы).png)

---

## Как говорить асимптотические оценки на английском

- Сама нотация так и говорится - Big-O(h)
- Как давать оценки:
    - `O(1)` - big oh of one; constant time
    - `O(log n)` - big oh of log n; logarithmic
    - `O(n)` - big oh of N; linear time
    - `O(n^2)` - big oh of n squared; quadratic
    - `O(2^n)` - big oh of power of two; exponential
- Случаи: worst case, best case, average case

---

## Другое

- Полезный сайтик с оценками всех популярных алгоритмов - https://www.bigocheatsheet.com/
- В папке doc есть файл `o_O.pdf` - это конспект уроков линейной алгебры и математического анализа из ВШЭ