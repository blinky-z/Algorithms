# Хэширование

**Хеширование** - это отображение некоторого множества объектов в множество чисел. Данная техника используется в хэш
таблице, которая использует хэш значение ключа для доступа к нужному бакету. Более того, хэширование позволяет
унифицировать способ доступа к произвольным данным.

# Хэш-таблица

Для начала рассмотрим структуру данных хэш-таблицу, которая по сути является синонимом хеширования.

## Описание

**Хэш-таблица (Hash table, hash map)** - это структура данных, имплементирующая интерфейс ассоциативного массива (мапы)
и позволяющая маппить ключ к значению.

Особенность хэш-таблицы в том, что в среднем случае она поддерживает доступ к ключу за константное время.

## Имплементация

Ключевая идея хэш-таблицы заключается в двух вещах:

- Использование массива для хранения пар (bucket array)
- Хэширование

Элементы массива называются бакетами (bucket). Хэш-функция используется для распределения пар в массиве бакетов (bucket
array).

Имея ключ, индекс бакета вычисляется следующим образом:

```
hash = hashfunc(key)
index = hash % array_size
```

Последнее выражение, взятие хэша по модулю размера массива, требуется потому, что хэш функция может выдать значение за
пределами массива. Такое значение замапить невозможно, если только не выделить массив такого размера, что он покрыл бы
все возможные выходные значения хеш функции. Понятно, что выделять массив такого размера совершенно не эффективно,
поэтому самым тривиальным решением остается взять хэш по модулю N, где N - размер массива.

Таким образом, имея ключ и хэш-функцию, мы можем вычислить индекс для расположения пары в нужной ячейке массива бакетов
и дальнейшего доступа к ней.

## Разрешение коллизий

При использовании хэш функций обязательно возникают *коллизии*. Коллизия хэш-функции - это случай, когда для двух
различных блоков данных мы получаем один и тот хэш или индекс. Это плохо, так как при попытке вставить новую пару, мы
можем наткнуться на уже существующую.

Решается эта проблема разными способами. Но перед тем, как перейти к этим способам, нужно рассказать об *открытом* и
*закрытом* хешировании.

## Open Hashing

**Открытое хеширование (open hashing)** - это когда элемент хранится не прямо в ячейке, указанном хэш значением, а в
списке.

Также это называется **Closed addressing** - это когда адрес (location) элемента определяется полностью лишь одним хэш
значением ключа.

Почему же эти два понятия так отличаются: в одном open, в другом closed? Дело в том, что *open* означает, что у нас нет
строгих гарантий на что-либо, а *closed* наоборот - означает что строгие гарантии есть.

То есть, open hashing - потому что объекты на самом деле не хранятся в массиве напрямую, а хранятся они в списке. Но
closed addressing потому, что адрес бакета определяется строго значением хэша.

### Separate Chaining

**Метод цепочек (separate chaining)** - это вид открытого хеширования. Он подразумевает хранение не единственной пары в
бакете, а списка пар. В качестве структуры данных обычно используется односвязный список, но может быть и Binary Search
Tree, если элементов слишком много и линейный поиск может занять много времени.

При поиске значения мы находим нужный бакет, как и раньше, а затем просто проходимся по всем парам в списке и сравниваем
искомый ключ с ключом в каждой паре, т.е. нужно, чтобы была корректно определена операция `equals`. Как только нашли
ключ, возвращаем эту пару.

При вставке же новой пары с таким же индексом мы просто добавляем эту пару в конец списка.

Понятно, что время для нахождения нужной пары увеличивается до времени нахождения ячейки массива (константное) + времени
нахождения ключа в списке.

## Closed Hashing

**Закрытое хеширование (closed hashing)** - это парадигма разрешения коллизий, которая использует *
probing* для нахождения следующей свободной ячейки массива.

Этот метод также известен как *Open addressing*, потому что здесь индекс бакета не определяется строго хэш значением, а
зависит от данных в таблице. Но closed hashing потому, что мы не выходим за рамки хеш таблицы - все элементы напрямую
хранятся в бакетах, а не списках.

Варианты разрешения коллизий типа closed hashing:

- Linear Probing
- Quadratic Probing
- Double Hashing
- и другие

Здесь можно почитать и о методе цепочек, и о других closed hashing методах разрешения
коллизий https://neerc.ifmo.ru/wiki/index.php?title=%D0%A0%D0%B0%D0%B7%D1%80%D0%B5%D1%88%D0%B5%D0%BD%D0%B8%D0%B5_%D0%BA%D0%BE%D0%BB%D0%BB%D0%B8%D0%B7%D0%B8%D0%B9

### Linear Probing

Если случается коллизия, т.е. ячейка с индексом i занята, то мы ищем следующую ячейку массива следующим образом:
*i + 1, i + 2, i + 3...*. Эта последовательность перебора называется *Probe sequence*.

Однако, существует проблема: такой метод создает длинные последовательности занятых бакетов. Когда мы будем добавлять
новые ключи, велик шанс, что мы попадем в одну из ячеек последовательности и еще более усугубим проблему, удлинив эту
последовательность. Все это влечет за собой увеличение среднего времени поиска и вставки. Этот феномен
называется *[Primary clustering](https://en.wikipedia.org/wiki/Primary_clustering)*.

Этот вариант обладает хорошей утилизацией кэша, но страдает от clustering. Поиск в таком варианте такой же - просто идем
по последовательности и сравниваем искомый ключ с текущим.

### Quadratic Probing

Если случается коллизия, т.е. ячейка с индексом i занята, то мы ищем следующую ячейку массива следующим образом:
*i + 1^2, i + 2^2, i + 3^2...*

Такой способ оставляет большие свободные промежутки между занятыми ячейками и избегает primary clustering.

Однако, если два ключа мапятся в один и тот же индекс, то второй ключ пойдет по тем же самым шагам, что и первый ключ.
Если это происходит слишком часто (из-за плохой хеш функции), то длинные последовательности все равно образуются и
производительность падает. Да, в этом варианте сложнее попасть в индекс занятой последовательности, так как элементы
идут не подряд, но это все равно может случиться. Этот феномен называется *Secondary Clustering*, и он является
обобщением Primary Clustering.

Clustering происходит потому, что probe sequence не зависит от ключа - мы просто перебираем 1, 2, 3.., поэтому другие
элементы могут пойти по этой же последовательности. Поэтому наверняка должен быть лучший способ - и он есть, давайте
рассмотрим далее.

Этот вариант не обладает такой хорошей утилизацией кеша, как Linear Probing, но и меньше страдает от clustering, то есть
является чем-то средним между linear probing и double hashing.

### Double hashing

Самый продвинутый вариант. В этом методе разрешения коллизий используется еще одна хеш функция для определения probe
sequence. Допустим, мы замапили ключ в индекс i, а вторая хеш функция дает результат h2(key)=j, то последовательность
будет следующей: `i + 1*j, i + 2*j, i + 3*j`.

Этот вариант совсем плох в плане утилизации кеша, но зато избегает как primary, так и secondary clustering.

## Сравнение Open vs Closed Hashing

1. Максимально возможный Load Factor: open - бесконечный, closed - 1, потому что в closed hashing мы храним элементы
   напрямую в ячейках массива
2. Размер внутреннего массива у Closed Hashing должен быть сильно больше, так как ключи лежат напрямую в массиве
3. Утилизация кеша в closed hashing лучше, так как элементы лежат линейно в памяти
4. Closed hashing работает лучше, если набор исходных элементов известен заранее
5. Clustering отсутствует у обоих, если в closed hashing использовать double hashing

## Lazy Deletion

**Lazy Deletion** - это способ удаления ключей из таблицы, использующей open addressing. В этом способе удаление
элемента происходит путем помечания ключа как удаленного, вместо того чтобы удалять его полностью. Тогда эти ячейки
считаются как пустые при вставке и как занятые при поиске.

Проблема с этим способом такая, что когда количество удалений/вставок увеличивается, стоимость поиска увеличивается.
Чтобы улучшить время, мы сделаем вот такой трюк: когда какой-нибудь элемент был найден при поиске, то этот элемент
перемещается в самую первую встретившуюся в текущем probing sequence ячейку, которая была помечена как удаленная.

Весь смысл *Lazy Deletion* в том, чтобы при удалении не искать элемент, который можно переместить в освободившуюся
ячейку, а делать это при поиске элемента, таким образом улучшая время удаления.

## Идеальная хэш-функция

Когда набор значений, которые требуется замапить, известен заранее, мы можем использовать *идеальную хэш-функцию*.

**Идеальная хэш-функция (Perfect Hash Function)** - это такая хеш-функция, которая преобразует заранее известное
статическое множество ключей в диапазон целых чисел без коллизий, т.е. один ключ соответствует только одному уникальному
значению. А если количество результирующих значений такое же как и количество входящих ключей, такая функция
называется *минимальной идеальной хеш-функцией (minimal perfect hash function)*. С математической точки зрения такая
функция является *инъекцией (injection)*.

Использование идеальной хэш-функции позволяет нам:

- Добиться константного времени доступа в худшем случае, так как список больше не нужен
- Оптимизировать расходы на память, позволяя не хранить ключ, так как нам больше нет необходимости его сравнивать при
  коллизиях, так как коллизий то больше и нет

## Load Factor

**Load Factor** - это отношение количества пар, помещенных в хэш-таблицу, к количеству бакетов внутреннего массива (
размеру массива).

С возрастанием данной величины возрастает и количество коллизий, так как остается все меньше свободных бакетов, а при
превышении значения в 1 свободных бакетов совсем не остается, а значит возрастает и среднее время доступа, переставая
быть константным. Обычно, при невысоких требованиях доступности (reliability), решением является увеличение размера
таблицы в 2 раза при достижении некоторого load factor. В таком случае нам также требуется перераспределить все пары (
рехешировать).

## Анализ сложности

Проанализируем время выполнения операций и занимаемое хэш-таблицой место.

Место, занимаемое хэш-таблицей всегда зависит от количества записей - поэтому O(n).

Средний случай - это когда load factor не высокий, коллизий мало и в среднем в каждом бакете находится только по одной
паре

- **Поиск:** O(1) - требуется обратиться только к ячейке массива, что всегда происходит за константное время
- **Вставка:** O(1) - вставить новый элемент в пустой список
- **Обновление:** O(1) - найти элемент в списке из 1 пары и обновить в нем значение
- **Удаление:** O(1) - требуется обратиться к ячейке массива + удаление элемента из списка размером 1

Худший случай - это когда все элементы мапятся только в один бакет из-за плохой хэш-функции. Тогда:

- **Поиск:** O(N) - необходимо пройтись по всему списку
- **Вставка:** O(1) - требуется обратиться к ячейке массива + вставить новый элемент в конец списка
- **Обновление:** O(N) - необходимо пройти по всему массиву за O(n) + заменить значение в паре за O(1)
- **Удаление:** O(1) - необходимо пройтись по всему списку + удалить элемент. Удаление в односвязном списке - это O(1),
  но если используется другая структура данных, нужно смотреть на ее время удаления. Однако, ассимптотически операция
  удаления все равно будет работать за O(n), так как я еще не встречал такой структуры данных, удаление из которой
  заняло бы больше чем линейное время, а поиск O(n) + удаление O(n) = O(n)

---

# Консистентное хеширование

Здесь поговорим о консистентном хешировании.

## Distributed Hashing

Перед тем как далее обсуждать консистентное хеширование, давайте поговорим о Distributed Hashing.

В некоторых ситуациях мы хотим разделить хэш таблицу на несколько частей, которые хостятся на разных серверах. Одна из
мотиваций сделать это - это обойти ограничения по мощности и памяти одного компьютера, позволяя создать таблицу сколько
угодно большого размера.

В таком случае, ключи distributed между разными серверами. Это
называется **[Distributed Hash Table](https://en.wikipedia.org/wiki/Distributed_hash_table)**.

Как же нам сделать distribution? Самый простой и наивный путь - это просто взять модуль от числа серверов (размера пула)
. То есть: `server = hash(key) mod N, где N размер пула`. Получив по ключу нужный сервер, мы идем и достаем значение из
этого сервера, а если значение там еще не закешировано, то мы кладем значение на сервер. Первый раз мы получим cache
miss, но в следующий раз уже найдем это значение.

## Проблема рехеширования

Выше приведенная схема distribution простая и понятная. Однако, все это хорошо работает до того момента, пока не
изменится количество серверов, а это может быть, ведь сервера часто падают или добавляются новые. Тогда нам потребуется
перехешировать все ключи между серверами еще раз, ведь мы использовали оператор взятию по модулю от размера пула. А это
плохо, так как теперь ключи будут мапиться в новые сервера, где значения не закешированы, соответственно мы получим
много cache miss, что ухудшит производительность.

## Консистентное хеширование

Так как же нам решить проблему рехеширования? Очевидно, нам нужен такой алгоритм хеширования, который не зависит
напрямую от количества серверов. Здесь нам и приходит на помощь консистентное хеширование.

**Консистентное хеширование** позволяет нам распределять ключи так, что при изменении количества бакетов будет
рехешировано в среднем только K/N ключей, где K - количество ключей, а N - количество бакетов после изменения (
количество серверов, размер пула). Для сравнения, в большинстве стандартных имплементаций хеш-таблиц требуется
рехешировать почти все ключи.

## Hash Ring

Один из популярных алгоритмов консистентного хеширования - это *Hash Ring*, который назначает серверам и ключам позицию
на абстрактном кольце (circle), или, как его называют, hash ring.

Итак, как же работает эта схема? Представим, что все выходные значения хеш функции замаплены на кольцо, где может быть
расположено `[0; 2^32-1]` значений, то есть диапазон выходного значения хеш функции, что является, по сути, размером
Integer.

Для каждого сервера вычисляется hash code и на окружности располагают сервера в получившиеся значения. Когда приходят
новые ключи, то для них также вычисляется hash code. Теперь мы можем замапить ключ в сервер, просто взяв ближайший
сервер, который встретится, если двигаться от hash code ключа по окружности по часовой стрелке. По сути, нам нужен
сервер "successor" от хэш кода ключа, поэтому эффективно для хранения серверов использовать Binary Search Tree
(*SortedMap* в Java).

![Consistent Hashing - Hash Ring](https://upload.wikimedia.org/wikipedia/commons/thumb/7/7a/Consistent_hashing.pdf/page1-800px-Consistent_hashing.pdf.jpg)

Удаление сервера:

- При выходе (удалении) какого либо сервера из строя потребуется переназначить только те ключи, которые мапились в
  данный сервер или его виртуальные ноды. Тогда новым их сервером станет тот, который следовал после старого по часовой
  стрелке. А остальные ключи мапятся все в те же сервера, как и раньше - их не трогаем
- Самое шикарное при удалении то, что требуется переназначить всего лишь `1/(N-1)` от всех ключей, где (N-1) -
  количество серверов после удаления

Добавление сервера:

- При добавлении нового сервера потребуется переназначить те ключи, которые стали ближе к новому добавленному серверу,
  чем к тому, на который они мапились раннее. А остальные ключи мапятся все в те же сервера - их не трогаем
- Таким же образом, самое шикарное при добавлении то, что требуется переназначить всего лишь `1/(N+1)` от всех ключей,
  где (N+1) - количество серверов после добавления

Один из примеров distributed hashing - это *Memcached*, который поддерживает консистентное хеширование из коробки.

### Вес серверов

Чтобы распределить нагрузку с учетом мощности серверов, мы можем задать вес серверов. Имплементируется это с помощью так
называемых *Virtual Nodes*. Это сервера, которые существуют на Hash Ring только виртуально, и любой маппинг в них
отображается в действительный сервер.

При использовании виртуальных серверов на окружности размещается не один сервер, а такое количество виртуальных
серверов, равное весу физического сервера. Тогда для более мощного сервера в него будет мапиться пропорционально большее
количество ключей.

## Пример использования консистентного хеширования

Итак, давайте покажу на практике, как можно использовать консистентное хеширование. Я не буду использовать Hash Ring
алгоритм, а заиспользую *Jump Consistent Hash* алгоритм, который был изобретен в Google. Данный алгоритм делает очень
хорошую работу по равномерному распределению ключей: в среднем при изменении количества серверов потребуется
перехешировать 1/N от всех ключей. Однако, этот алгоритм требует последовательного именования шардов. То есть, алгоритм
плохо работает в случае, когда удаляется произвольный шард, и также плохо работает, когда добавляется новый шард в
произвольное место. Этот алгоритм требует, чтобы новые шарды добавлялись в конец ordered списка шардов и удалялись также
с конца списка. Вот здесь можно увидеть [white paper](https://arxiv.org/abs/1406.2294).

Теперь давайте заиспользуем этот алгоритм для шардирования. Допустим, у нас есть фиксированное количество шардов - 3
штуки, шарды пронумерованы от 0 до 2 и каждому шарду можно задать вес (количество Virtual Nodes). Тогда вычислить шард
по ключу можно следующим образом:

```java
public class ShardResolver {
    public static Shard getShard(@NotNull String senderId, @NotNull Collection<Shard> shards) {
        // вычисляем hashCode
        HashCode hashCode = Hashing.murmur3_128().hashString(senderId, Charsets.UTF_8);

        // вычисляем индекс для элемента, используя консистентное хеширование
        // функция consistentHash принимает вычисленный хеш код и количество бакетов
        // здесь количество бакетов - это количество виртуальных нод
        int totalWeight = shards.stream()
                .map(Shard::getWeight)
                .reduce(0, Integer::sum);
        int weightedKey = Hashing.consistentHash(hashCode, totalWeight);

        // ищем ближайший к вычисленному индексу шард по часовой стрелке (successor)
        // шарды идут по порядку их расположения на Hash Ring
        int weightAccum = 0;
        for (Shard shard : shards) {
            if (weightedKey <= weightAccum) {
                return shard;
            }
            weightAccum += shard.getWeight();
        }
    }
}
```

ID шарда вычисляется следующим образом:

1. Берется ключ и по нему считается `hashCode`
2. Для данного хэш значения применяется алгоритм консистентного хеширования для вычисления ID
   шарда: `consistentHash(hash, buckets)`. Первым параметром передается вычисленное на 1-ом шаге значение хэша, а вторым
    - общий вес шардов. На выходе мы получаем значение *weightedKey* из промежутка [0; W), где W - общий вес шардов
3. Результирующее значение из промежутка [0; W) приводится к нужному шарду. Делается это так: мы проходимся по всем
   шардам, начиная с первого, и суммируем текущий общий вес шардов. Если результирующее значение *weightedKey* меньше
   текущего общего веса *weightAccum*, то мы нашли нужный шард, иначе ищем дальше. Например, даны веса шардов: 3, 1, 4.
   Если алгоритм консистентного хеширования принимает на вход ID пользователя и число 8 (общий вес шардов) и выдает {0,
   1, 2}, то этой 1-ый шард (на этой итерации текущий общий вес равен 3), если выдает {3}, то это 2-ой шард (на этой
   итерации текущий общий вес равен 4), если выдает {4, 5, 6, 7}, то это третий шард (на этой итерации общий вес шардов
   равен 8).

Для первичного хэширования используется алгоритм [MurmurHash3](https://en.wikipedia.org/wiki/MurmurHash), который
является *детерминированным*, то есть возвращает одинаковый результат для одних и тех же входных данных. Используемая
для консистентного хеширования библиотека -
это [Google Guava](https://guava.dev/releases/21.0/api/docs/com/google/common/hash/Hashing.html).

Посмотреть полный код можно в пакете `me.progbloom.hashing.jump`.

## Почему консистентное хеширование не распространено в обычных хеш таблицах?

Почему же консистентное хеширование активно применяется в распределенных системах, но не применяется в хеш таблицах?
Дело в том, что проблема рехеширования присутствует только при шардировании - ведь если мы рехешируем все ключи в другие
сервера, то потеряем доступ к ранее сохраненных данным, поэтому там очень важно иметь алгоритм, который предлагает
минимальное количество рехеширований.

В случае с хеш мапой же, она не является распределенной, а лежит в памяти одного инстанса приложения и нам совершенно не
оправданно использовать такие сложные алгоритмы хеширования, ведь мы можем спокойно рехешировать все ключи и не получить
ухудшения работы приложения.